---
title: |
 Advanced Topics in Statistics  
 Classification in R and Python
  
date: "03/02/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this practical we will cover the following topics:

* Logistic regression



## Classification using logistic regression

**Quick recap** - for more details see Classification slides on the ELE page.

Imagine that an observation consists of some explanatory variables $X_1,X_2,\dots,X_p$ and a binary label $Y$ (that is $Y$ is either 1 or 0). When doing classification, we want to find a method that assigns one label to each observation in a way that the method correctly predicts not only the observed labels, but it also performs well when assingning labels to new observations.

Logistic regression is one method that can used to classify observations. It uses the form
\[
p:=\mathbb{P}(Y=1|X_1,\dots,X_p)=\frac{\mathrm{e}^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}{1+\mathrm{e}^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}},
\]
that is we first estimate the probability that the observation (with explanatory variables $X_1,\dots,X_p$) is of category 1.
The coefficients $\beta_0,\beta_1,\dots,\beta_p$ are estimated using the observed data.

Then, in order to predict the label of new observations, we choose a threshold $p*$, and assign label 1 if the value $p$ of the new observation is above the threshold $p*$ (and assing label 0 otherwise).

There are a couple of measures of testing a modelâ€™s classification performance. Using our data we can construct the following *confusion matrix*:

\begin{center}
\begin{tabular}{c | c c}
& Obs:0 & Obs:1\\\hline
Pred:0 & a & b\\
Pred:1 & c & d
\end{tabular}
\end{center}

The *accuracy* is the fraction of correct classifications, that is $(a+d)/(a+b+c+d)$. 
Furthermore we can define the following measures:

* Sensitivity: proportion of true positives predicted as such, $a/(b+d)$.

* Specificity: proportion of true negatives predicted as such, $a/(a+c)$.

* Positive predictive value: proportion of predicted positives
that are actually correct, $d/(c+d)$.

* Negative predictive value: proportion of predicted
negatives that are actually correct, $a/(a+b)$.


**Breast Cancer data**

In this example we will attempt to classify tumors as being benign (0) or malignant (1). The data is readily available in Python, in R you will have to read in the `cancer.csv` dataset, which you can find on the ELE page. The data consists of 30 explanatory variables (different measurements of the tumor), and the label indicating whether the tumor is benign or malignant.

Since we only have one dataset, to test model performance we will randomly split to data, and build the model on 75\% of available, and test the model on the remaining 25\%.

\textcolor{red}{\textbf{R:}}

Load the necessary packages and the data:
```{r,eval=FALSE}
# The caret and e1071 package is required for using the confusionMatrix call
library(dplyr)
library(caret)
library(e1071)

cancer <- read.csv("cancer.csv",header=FALSE)
```

Look at the data (there is a lot so just the first three variables here, try altering this to get a sense of the data)
```{r}
y <- cancer[,31]
v1 <- cancer[,1]
v2 <- cancer[,2]
v3 <- cancer[,3]
# Basic Scatterplot Matrix
pairs(~y+v1+v2+v3,data=cancer,
   main="Simple Scatterplot Matrix")
```


Next we manually define a function that will split the data into training set and test set, and use this function to make the split:
```{r,eval=FALSE}
# Define function that will split the data into training and test sets
trainTestSplit <- function(df,trainPercent,seed1){
## Sample size percent
smp_size <- floor(trainPercent/100 * nrow(df))
## set the seed 
    set.seed(seed1)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train_ind
}

# Split as training and test sets
train_ind <- trainTestSplit(cancer,trainPercent=85,seed=123)
train <- cancer[train_ind, ]
test <- cancer[-train_ind, ]

```

Then we fit the glm (note that the labels are in the last column of the dataset - V31).

Explore differing numbers of the tumor measurements to aid with classification
```{r,eval=FALSE}
# Fit a generalized linear logistic model
# Note that the last column (V31) contains the labels

# This fits all the 30 measurements
#fit <- glm(V31~.,family=binomial,data=train,control = list(maxit = 50))

# This fits V1
#fit <- glm(V31~V1,family=binomial,data=train,control = list(maxit = 50))

# This fits V2
#fit <- glm(V31~V2,family=binomial,data=train,control = list(maxit = 50))

#This fits V1 + V2 + V3
fit <- glm(V31~V1+V2+V3,family=binomial,data=train,control = list(maxit = 50))


```

Explore the model output

```{r}
fitted.mode1 <- fitted(fit)

plot(train[,1],fitted(fit))
plot(train[,2],fitted(fit))
plot(train[,3],fitted(fit))
plot(train[,31],fitted(fit))

```

Next we check the performance of the trained model. Here we use the threshold $p*=0.5$.
```{r,eval=FALSE}
# Predict the output from the model
a=predict(fit,newdata=train,type="response")
# Set response >0.5 as 1 and <=0.5 as 0
b=ifelse(a>0.5,1,0)

# Compute the confusion matrix for training data
confusionMatrix(as.factor(b),as.factor(train$V31))
```

Finally we use the trained model to make predictions for the test dataset. Again, we use the threshold $p*=0.5$.
```{r,eval=FALSE}

m=predict(fit,newdata=test,type="response")
n=ifelse(m>0.6,1,0)
# Compute the confusion matrix for test output
confusionMatrix(as.factor(n),as.factor(test$V31))
```

\textcolor{blue}{\textbf{Python:}}
Load necessary packages and the data:
```{r,eval=FALSE}
# Import model we want to use
# In sklearn, all machine learning models are implemented as Python classes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
# Make an instance of the model, all parameters not specified will be set to their defaults
# This way we don't have to save the outcomes from model fit and prediction separately
#   but logisticRegr already has these variables, so it will be enough to update them
logisticRegr = LogisticRegression()

from sklearn.metrics import confusion_matrix
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
print(cancer.data.shape)
print(cancer.target.shape)
```
From the dimensions of the data and target we can conclude that there are 569 patients; and 30 potential explanatory variables.
The labels indicate whether the patient had breast cancer or not.

Recall that we would like to fit a logistic regression to estimate the probability that a given patient has cancer (the tumor is malignant) based on the available explanatory variables.

First we extract the explanatory variables and the labels, then split the data into a training set and a test set. Python has a built in function for this, thus here we don't have to write our own.
```{r,eval=FALSE}
(X_cancer, y_cancer) = load_breast_cancer(return_X_y=True)
# We split the data into training and test sets
# We do this to make sure that after we train our classification algorithm,
# it is able to generalise well to new data.
X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,
                                                    test_size=0.25, random_state=0)
```

Next, fit the model and make the predictions
```{r,eval=FALSE}
# Call the Logisitic Regression function
logisticRegr.fit(X_train, y_train)

# Predict label for the first test data
logisticRegr.predict(X_test[0].reshape(1, -1))
# Predict for all the test data
predictions = logisticRegr.predict(X_test)
```

Finally we look at model performance.
```{r,eval=FALSE}
## Measure model performance
# Here we are going to use accuracy to measure model performance
# Recall that accuracy is the fraction of correct model predictions

# Use score method to get accuracy of the model
score = logisticRegr.score(X_test, y_test)
print(score)
# Our accuracy was 95.8%


# Compute and print confusion matrix
confusion = confusion_matrix(y_test, predictions)
print(confusion)

from sklearn.metrics import accuracy_score, precision_score
print('Accuracy: {:.2f}'.format(accuracy_score(y_test, predictions)))
print('Precision: {:.2f}'.format(precision_score(y_test, predictions)))
```

